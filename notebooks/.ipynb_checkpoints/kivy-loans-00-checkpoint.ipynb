{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ednalyndedios/.pyenv/versions/3.8.2/envs/kln382/lib/python3.8/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ednalyndedios/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ednalyndedios/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# to manipulate dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# natural language processing: n-gram ranking\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for natural language processing: named entity recognition\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# for natural language processing: sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['kiva']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data; text data MUST be in the first column\n",
    "df = pd.read_csv('../data/input/kiva_loans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>funded_amount</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>activity</th>\n",
       "      <th>sector</th>\n",
       "      <th>use</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>currency</th>\n",
       "      <th>partner_id</th>\n",
       "      <th>posted_time</th>\n",
       "      <th>disbursed_time</th>\n",
       "      <th>funded_time</th>\n",
       "      <th>term_in_months</th>\n",
       "      <th>lender_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>borrower_genders</th>\n",
       "      <th>repayment_interval</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>653051</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Fruits &amp; Vegetables</td>\n",
       "      <td>Food</td>\n",
       "      <td>To buy seasonal, fresh fruits to sell.</td>\n",
       "      <td>PK</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Lahore</td>\n",
       "      <td>PKR</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2014-01-01 06:12:39+00:00</td>\n",
       "      <td>2013-12-17 08:00:00+00:00</td>\n",
       "      <td>2014-01-02 10:06:32+00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>irregular</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>653053</td>\n",
       "      <td>575.0</td>\n",
       "      <td>575.0</td>\n",
       "      <td>Rickshaw</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>to repair and maintain the auto rickshaw used ...</td>\n",
       "      <td>PK</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Lahore</td>\n",
       "      <td>PKR</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2014-01-01 06:51:08+00:00</td>\n",
       "      <td>2013-12-17 08:00:00+00:00</td>\n",
       "      <td>2014-01-02 09:17:23+00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female, female</td>\n",
       "      <td>irregular</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>653068</td>\n",
       "      <td>150.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>To repair their old cycle-van and buy another ...</td>\n",
       "      <td>IN</td>\n",
       "      <td>India</td>\n",
       "      <td>Maynaguri</td>\n",
       "      <td>INR</td>\n",
       "      <td>334.0</td>\n",
       "      <td>2014-01-01 09:58:07+00:00</td>\n",
       "      <td>2013-12-17 08:00:00+00:00</td>\n",
       "      <td>2014-01-01 16:01:36+00:00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6</td>\n",
       "      <td>user_favorite, user_favorite</td>\n",
       "      <td>female</td>\n",
       "      <td>bullet</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>653063</td>\n",
       "      <td>200.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>Embroidery</td>\n",
       "      <td>Arts</td>\n",
       "      <td>to purchase an embroidery machine and a variet...</td>\n",
       "      <td>PK</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Lahore</td>\n",
       "      <td>PKR</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2014-01-01 08:03:11+00:00</td>\n",
       "      <td>2013-12-24 08:00:00+00:00</td>\n",
       "      <td>2014-01-01 13:00:00+00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>irregular</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>653084</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Milk Sales</td>\n",
       "      <td>Food</td>\n",
       "      <td>to purchase one buffalo.</td>\n",
       "      <td>PK</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Abdul Hakeem</td>\n",
       "      <td>PKR</td>\n",
       "      <td>245.0</td>\n",
       "      <td>2014-01-01 11:53:19+00:00</td>\n",
       "      <td>2013-12-17 08:00:00+00:00</td>\n",
       "      <td>2014-01-01 19:18:51+00:00</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>monthly</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  funded_amount  loan_amount             activity          sector  \\\n",
       "0  653051          300.0        300.0  Fruits & Vegetables            Food   \n",
       "1  653053          575.0        575.0             Rickshaw  Transportation   \n",
       "2  653068          150.0        150.0       Transportation  Transportation   \n",
       "3  653063          200.0        200.0           Embroidery            Arts   \n",
       "4  653084          400.0        400.0           Milk Sales            Food   \n",
       "\n",
       "                                                 use country_code   country  \\\n",
       "0            To buy seasonal, fresh fruits to sell.            PK  Pakistan   \n",
       "1  to repair and maintain the auto rickshaw used ...           PK  Pakistan   \n",
       "2  To repair their old cycle-van and buy another ...           IN     India   \n",
       "3  to purchase an embroidery machine and a variet...           PK  Pakistan   \n",
       "4                           to purchase one buffalo.           PK  Pakistan   \n",
       "\n",
       "         region currency  partner_id                posted_time  \\\n",
       "0        Lahore      PKR       247.0  2014-01-01 06:12:39+00:00   \n",
       "1        Lahore      PKR       247.0  2014-01-01 06:51:08+00:00   \n",
       "2     Maynaguri      INR       334.0  2014-01-01 09:58:07+00:00   \n",
       "3        Lahore      PKR       247.0  2014-01-01 08:03:11+00:00   \n",
       "4  Abdul Hakeem      PKR       245.0  2014-01-01 11:53:19+00:00   \n",
       "\n",
       "              disbursed_time                funded_time  term_in_months  \\\n",
       "0  2013-12-17 08:00:00+00:00  2014-01-02 10:06:32+00:00            12.0   \n",
       "1  2013-12-17 08:00:00+00:00  2014-01-02 09:17:23+00:00            11.0   \n",
       "2  2013-12-17 08:00:00+00:00  2014-01-01 16:01:36+00:00            43.0   \n",
       "3  2013-12-24 08:00:00+00:00  2014-01-01 13:00:00+00:00            11.0   \n",
       "4  2013-12-17 08:00:00+00:00  2014-01-01 19:18:51+00:00            14.0   \n",
       "\n",
       "   lender_count                          tags borrower_genders  \\\n",
       "0            12                           NaN           female   \n",
       "1            14                           NaN   female, female   \n",
       "2             6  user_favorite, user_favorite           female   \n",
       "3             8                           NaN           female   \n",
       "4            16                           NaN           female   \n",
       "\n",
       "  repayment_interval        date  \n",
       "0          irregular  2014-01-01  \n",
       "1          irregular  2014-01-01  \n",
       "2             bullet  2014-01-01  \n",
       "3          irregular  2014-01-01  \n",
       "4            monthly  2014-01-01  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes null values\n",
    "df = df.loc[df.use.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id', 'funded_amount', 'loan_amount', 'activity', 'sector', 'use', 'country_code', 'borrower_genders', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666973, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(5000, random_state=493).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "  \"\"\"\n",
    "  A simple function to clean up the data. All the words that\n",
    "  are not designated as a stop word is then lemmatized after\n",
    "  encoding and basic regex parsing are performed.\n",
    "  \"\"\"\n",
    "  wnl = nltk.stem.WordNetLemmatizer()\n",
    "  stopwords = nltk.corpus.stopwords.words('english')+ ADDITIONAL_STOPWORDS\n",
    "  text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "def get_bigrams(content):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a dataframe with the top 20 bigrams.\n",
    "    \"\"\"\n",
    "    bigrams = (pd.Series(nltk.ngrams(content, 2)).value_counts())[:10].to_frame().reset_index()\n",
    "    bigrams.columns=['bigram', 'count']\n",
    "    return bigrams\n",
    "\n",
    "def get_trigrams(content):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a dataframe with the top 20 trigrams.\n",
    "    \"\"\"\n",
    "    trigrams = (pd.Series(nltk.ngrams(content, 3)).value_counts())[:10].to_frame().reset_index()\n",
    "    trigrams.columns=['trigram', 'count']\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to a list of clean tokens\n",
    "content = clean(''.join(str(df.use.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: push to S3\n",
    "# exports to a flat file\n",
    "get_bigrams(content).to_csv('../data/output/output_data_ngram_bigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: push to S3\n",
    "# exports to a flat file\n",
    "get_trigrams(content).to_csv('../data/output/output_data_ngram_trigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(entities, ent_type):\n",
    "    group_list = []\n",
    "    for entity in entities:\n",
    "        if entity.label_ == ent_type:\n",
    "            group_list.append(entity.text)\n",
    "    df_entities = pd.DataFrame(Counter(group_list).most_common(20))\n",
    "    df_entities.columns=['entity', 'count']\n",
    "    return df_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-baab02881c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_sm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tagger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parser\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"textcat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/kln382/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/kln382/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
